{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# tensorflow is a library for fast tensor manipulation\n",
    "from keras.datasets import imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "# num_words means you'll only keep the top 10,000 most frequently occuring words in the training data\n",
    "# rare words will be discarded\n",
    "# this allows to work with vector data of manageable size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 2,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 2,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 2,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 2,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 2,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 2,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([max(sequence) for sequence in train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict(\n",
    "    [(value, key) for (key, value) in word_index.items()])\n",
    "decoded_review = ' '.join(\n",
    "    [reverse_word_index.get(i-3, '?') for i in train_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "#word_index\n",
    "#type (train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type (decoded_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the network\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential() # Sequential is important\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tells the network what optimizer to use\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 10000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 3s 171us/step - loss: 0.4997 - acc: 0.7921 - val_loss: 0.3752 - val_acc: 0.8698\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 2s 125us/step - loss: 0.2969 - acc: 0.9037 - val_loss: 0.2995 - val_acc: 0.8911\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 2s 124us/step - loss: 0.2162 - acc: 0.9285 - val_loss: 0.3087 - val_acc: 0.8719\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 2s 123us/step - loss: 0.1738 - acc: 0.9437 - val_loss: 0.2829 - val_acc: 0.8844\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 2s 125us/step - loss: 0.1415 - acc: 0.9543 - val_loss: 0.2864 - val_acc: 0.8854\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 2s 125us/step - loss: 0.1143 - acc: 0.9655 - val_loss: 0.3151 - val_acc: 0.8773\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 2s 125us/step - loss: 0.0975 - acc: 0.9712 - val_loss: 0.3138 - val_acc: 0.8841\n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 2s 124us/step - loss: 0.0803 - acc: 0.9765 - val_loss: 0.3863 - val_acc: 0.8654\n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 2s 124us/step - loss: 0.0658 - acc: 0.9818 - val_loss: 0.3647 - val_acc: 0.8780\n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 2s 124us/step - loss: 0.0561 - acc: 0.9849 - val_loss: 0.3854 - val_acc: 0.8793\n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 2s 125us/step - loss: 0.0439 - acc: 0.9893 - val_loss: 0.4159 - val_acc: 0.8776\n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 2s 123us/step - loss: 0.0382 - acc: 0.9919 - val_loss: 0.4539 - val_acc: 0.8692\n",
      "Epoch 13/20\n",
      "15000/15000 [==============================] - 2s 125us/step - loss: 0.0300 - acc: 0.9931 - val_loss: 0.4712 - val_acc: 0.8734\n",
      "Epoch 14/20\n",
      "15000/15000 [==============================] - 2s 123us/step - loss: 0.0251 - acc: 0.9941 - val_loss: 0.5050 - val_acc: 0.8717\n",
      "Epoch 15/20\n",
      "15000/15000 [==============================] - 2s 123us/step - loss: 0.0197 - acc: 0.9963 - val_loss: 0.5323 - val_acc: 0.8712\n",
      "Epoch 16/20\n",
      "15000/15000 [==============================] - 2s 123us/step - loss: 0.0161 - acc: 0.9973 - val_loss: 0.5660 - val_acc: 0.8687\n",
      "Epoch 17/20\n",
      "15000/15000 [==============================] - 2s 126us/step - loss: 0.0136 - acc: 0.9981 - val_loss: 0.5967 - val_acc: 0.8677\n",
      "Epoch 18/20\n",
      "15000/15000 [==============================] - 2s 126us/step - loss: 0.0100 - acc: 0.9987 - val_loss: 0.6289 - val_acc: 0.8672\n",
      "Epoch 19/20\n",
      "15000/15000 [==============================] - 2s 125us/step - loss: 0.0110 - acc: 0.9980 - val_loss: 0.6591 - val_acc: 0.8665\n",
      "Epoch 20/20\n",
      "15000/15000 [==============================] - 2s 126us/step - loss: 0.0045 - acc: 0.9999 - val_loss: 0.6838 - val_acc: 0.8664\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Batch_size is the number of times it runs through the batch and adds it back\n",
    "history = model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acc', 'loss', 'val_acc', 'val_loss']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "[u'acc', u'loss', u'val_acc', u'val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Overtraining/overfeeding is where it memorizes the pattern that was set for the network\n",
    "# It memorizes the small patterns and if something falls into its pattern then it\n",
    "# gets labeled as that thing\n",
    "# You can add random noise to fix this problem. It creates a more broad outline so it doesn't\n",
    "# memorize patterns \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc)+1)\n",
    "\n",
    "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf() # clears the figure\n",
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 2s 99us/step - loss: 0.4732 - acc: 0.8217\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 2s 83us/step - loss: 0.2672 - acc: 0.9089\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 2s 83us/step - loss: 0.2033 - acc: 0.9287\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 2s 84us/step - loss: 0.1714 - acc: 0.9386\n",
      "25000/25000 [==============================] - 3s 133us/step\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential() # Sequential is important\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "             loss = 'binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.31153844117164614, 0.87572]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16415974],\n",
       "       [0.99985015],\n",
       "       [0.39135462],\n",
       "       ...,\n",
       "       [0.08186716],\n",
       "       [0.04870189],\n",
       "       [0.45567998]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.46884528],\n",
       "       [0.49825823],\n",
       "       [0.53285277],\n",
       "       [0.45565933],\n",
       "       [0.46207035],\n",
       "       [0.43713602],\n",
       "       [0.5295497 ],\n",
       "       [0.59822875],\n",
       "       [0.5308322 ],\n",
       "       [0.54123455],\n",
       "       [0.54123455],\n",
       "       [0.45730573],\n",
       "       [0.5457104 ],\n",
       "       [0.43750972],\n",
       "       [0.45604572],\n",
       "       [0.46333763],\n",
       "       [0.46207035],\n",
       "       [0.48629972],\n",
       "       [0.48065796],\n",
       "       [0.52088976],\n",
       "       [0.54603726],\n",
       "       [0.59822875],\n",
       "       [0.50719976],\n",
       "       [0.5064433 ],\n",
       "       [0.458501  ],\n",
       "       [0.47297212],\n",
       "       [0.54123455],\n",
       "       [0.54123455],\n",
       "       [0.5070361 ],\n",
       "       [0.51076496],\n",
       "       [0.41486254],\n",
       "       [0.42104274],\n",
       "       [0.43713602],\n",
       "       [0.49937066],\n",
       "       [0.4588991 ],\n",
       "       [0.52862924],\n",
       "       [0.5058681 ],\n",
       "       [0.45205435],\n",
       "       [0.48643237],\n",
       "       [0.4606963 ],\n",
       "       [0.47523627],\n",
       "       [0.47669426],\n",
       "       [0.49294508],\n",
       "       [0.47128662],\n",
       "       [0.54123455],\n",
       "       [0.43713602],\n",
       "       [0.5010422 ],\n",
       "       [0.46207035],\n",
       "       [0.47523627],\n",
       "       [0.52988863],\n",
       "       [0.45365775],\n",
       "       [0.4864186 ],\n",
       "       [0.47312817],\n",
       "       [0.4628559 ],\n",
       "       [0.49552912],\n",
       "       [0.4748173 ],\n",
       "       [0.48107827],\n",
       "       [0.47398955],\n",
       "       [0.5110139 ],\n",
       "       [0.51134866],\n",
       "       [0.48339885],\n",
       "       [0.50389194],\n",
       "       [0.48065796],\n",
       "       [0.46207035],\n",
       "       [0.5110139 ],\n",
       "       [0.56926525],\n",
       "       [0.52988863],\n",
       "       [0.48065796],\n",
       "       [0.46913004],\n",
       "       [0.5295497 ],\n",
       "       [0.47523627],\n",
       "       [0.5314845 ],\n",
       "       [0.48065796],\n",
       "       [0.41486254],\n",
       "       [0.4683198 ],\n",
       "       [0.52559304],\n",
       "       [0.5110139 ],\n",
       "       [0.5295497 ],\n",
       "       [0.43276483],\n",
       "       [0.49052304],\n",
       "       [0.5295497 ],\n",
       "       [0.51134866],\n",
       "       [0.5400755 ],\n",
       "       [0.46207035],\n",
       "       [0.51134866],\n",
       "       [0.47297212],\n",
       "       [0.45090908],\n",
       "       [0.48339885],\n",
       "       [0.49294508],\n",
       "       [0.48107827],\n",
       "       [0.4682665 ],\n",
       "       [0.44667614],\n",
       "       [0.49871898],\n",
       "       [0.43713602],\n",
       "       [0.54508114],\n",
       "       [0.46207035],\n",
       "       [0.5445471 ],\n",
       "       [0.51134866],\n",
       "       [0.48107827],\n",
       "       [0.46884528],\n",
       "       [0.48392358],\n",
       "       [0.5011221 ],\n",
       "       [0.53862625],\n",
       "       [0.48889711],\n",
       "       [0.5295497 ],\n",
       "       [0.54603726],\n",
       "       [0.59822875],\n",
       "       [0.50719976],\n",
       "       [0.4141285 ],\n",
       "       [0.47669426],\n",
       "       [0.52715904],\n",
       "       [0.51364046],\n",
       "       [0.5295497 ],\n",
       "       [0.4384232 ],\n",
       "       [0.5097069 ],\n",
       "       [0.48339885],\n",
       "       [0.5377893 ],\n",
       "       [0.48051104],\n",
       "       [0.55938905],\n",
       "       [0.48065796],\n",
       "       [0.4811362 ],\n",
       "       [0.43337497],\n",
       "       [0.49232143],\n",
       "       [0.45456043],\n",
       "       [0.4873215 ],\n",
       "       [0.50613695],\n",
       "       [0.43713602],\n",
       "       [0.50123924],\n",
       "       [0.44309375],\n",
       "       [0.48339885],\n",
       "       [0.49294508],\n",
       "       [0.47523627],\n",
       "       [0.4628559 ],\n",
       "       [0.4873215 ],\n",
       "       [0.48339885],\n",
       "       [0.47797   ],\n",
       "       [0.5311813 ],\n",
       "       [0.48392358],\n",
       "       [0.47037333],\n",
       "       [0.5018458 ],\n",
       "       [0.5305292 ],\n",
       "       [0.43713602],\n",
       "       [0.51134866],\n",
       "       [0.4966608 ],\n",
       "       [0.45604572],\n",
       "       [0.53789973],\n",
       "       [0.46207035],\n",
       "       [0.47365403],\n",
       "       [0.47128662],\n",
       "       [0.44772577],\n",
       "       [0.51610446],\n",
       "       [0.48107827],\n",
       "       [0.43713602],\n",
       "       [0.46893618],\n",
       "       [0.43561202],\n",
       "       [0.5732091 ],\n",
       "       [0.44309375],\n",
       "       [0.5472843 ],\n",
       "       [0.43713602],\n",
       "       [0.48514628],\n",
       "       [0.48065796],\n",
       "       [0.48643237],\n",
       "       [0.4588991 ],\n",
       "       [0.59822875],\n",
       "       [0.48107827],\n",
       "       [0.43320036],\n",
       "       [0.43337497],\n",
       "       [0.54001087],\n",
       "       [0.47823068],\n",
       "       [0.54720956],\n",
       "       [0.46207035],\n",
       "       [0.46287927],\n",
       "       [0.46207035],\n",
       "       [0.45171994],\n",
       "       [0.5305292 ],\n",
       "       [0.43713602],\n",
       "       [0.4915733 ],\n",
       "       [0.51134866],\n",
       "       [0.46779728],\n",
       "       [0.4446647 ],\n",
       "       [0.5108957 ],\n",
       "       [0.4507987 ],\n",
       "       [0.5063456 ],\n",
       "       [0.49252588],\n",
       "       [0.46295652],\n",
       "       [0.5100931 ],\n",
       "       [0.47037333],\n",
       "       [0.4954975 ],\n",
       "       [0.48107827],\n",
       "       [0.48889711],\n",
       "       [0.52559304],\n",
       "       [0.4628792 ],\n",
       "       [0.50268555],\n",
       "       [0.48088622],\n",
       "       [0.5728805 ],\n",
       "       [0.5484751 ],\n",
       "       [0.4569947 ],\n",
       "       [0.5796969 ],\n",
       "       [0.5121058 ],\n",
       "       [0.45635846],\n",
       "       [0.47312194],\n",
       "       [0.47523627],\n",
       "       [0.44319838],\n",
       "       [0.49407688],\n",
       "       [0.43713602],\n",
       "       [0.5346205 ],\n",
       "       [0.46207035],\n",
       "       [0.43713602],\n",
       "       [0.45548707],\n",
       "       [0.46207035],\n",
       "       [0.47523627],\n",
       "       [0.51134866],\n",
       "       [0.4610694 ],\n",
       "       [0.51364046],\n",
       "       [0.49207896],\n",
       "       [0.52559304],\n",
       "       [0.45565933],\n",
       "       [0.47037333],\n",
       "       [0.5108957 ],\n",
       "       [0.43787882],\n",
       "       [0.5058681 ],\n",
       "       [0.48339885],\n",
       "       [0.47037333],\n",
       "       [0.45129013],\n",
       "       [0.49601325],\n",
       "       [0.45129013],\n",
       "       [0.48339885],\n",
       "       [0.48107827],\n",
       "       [0.4289914 ],\n",
       "       [0.47312194],\n",
       "       [0.57896125],\n",
       "       [0.4343085 ],\n",
       "       [0.48643237],\n",
       "       [0.51610446],\n",
       "       [0.48107827],\n",
       "       [0.50719976],\n",
       "       [0.49652964],\n",
       "       [0.47312194],\n",
       "       [0.43713602],\n",
       "       [0.46884528],\n",
       "       [0.4910512 ],\n",
       "       [0.4602645 ],\n",
       "       [0.47523627],\n",
       "       [0.51134866],\n",
       "       [0.4610694 ],\n",
       "       [0.52559304],\n",
       "       [0.47823068],\n",
       "       [0.52179116],\n",
       "       [0.51134866],\n",
       "       [0.47297212],\n",
       "       [0.46207035],\n",
       "       [0.48065796],\n",
       "       [0.47906062],\n",
       "       [0.5732091 ],\n",
       "       [0.48065796],\n",
       "       [0.43152466],\n",
       "       [0.5445471 ],\n",
       "       [0.51351875],\n",
       "       [0.46654812],\n",
       "       [0.52289397],\n",
       "       [0.48107827],\n",
       "       [0.43152466],\n",
       "       [0.48107827],\n",
       "       [0.6217099 ],\n",
       "       [0.48156637],\n",
       "       [0.5796969 ],\n",
       "       [0.52559304],\n",
       "       [0.46654812],\n",
       "       [0.43713602],\n",
       "       [0.48953682],\n",
       "       [0.47312194],\n",
       "       [0.4954975 ],\n",
       "       [0.48051104],\n",
       "       [0.5308322 ],\n",
       "       [0.48833472],\n",
       "       [0.5457104 ],\n",
       "       [0.50342906],\n",
       "       [0.4682665 ],\n",
       "       [0.4446647 ],\n",
       "       [0.45896113],\n",
       "       [0.48323417],\n",
       "       [0.44973794],\n",
       "       [0.48107827],\n",
       "       [0.43713602],\n",
       "       [0.521338  ],\n",
       "       [0.52289397],\n",
       "       [0.51134866],\n",
       "       [0.48065796],\n",
       "       [0.3945839 ],\n",
       "       [0.4682665 ],\n",
       "       [0.43713602],\n",
       "       [0.41880763],\n",
       "       [0.521338  ],\n",
       "       [0.50389194],\n",
       "       [0.4891309 ],\n",
       "       [0.5150398 ],\n",
       "       [0.5253963 ],\n",
       "       [0.41880763],\n",
       "       [0.4938811 ],\n",
       "       [0.45548707],\n",
       "       [0.5150398 ],\n",
       "       [0.5109911 ],\n",
       "       [0.48107827],\n",
       "       [0.47381744],\n",
       "       [0.4937135 ],\n",
       "       [0.43616483],\n",
       "       [0.48065796],\n",
       "       [0.52862924],\n",
       "       [0.465976  ],\n",
       "       [0.45565933],\n",
       "       [0.4886482 ],\n",
       "       [0.59080464],\n",
       "       [0.4358034 ],\n",
       "       [0.48610386],\n",
       "       [0.48065796],\n",
       "       [0.51589394],\n",
       "       [0.45023474],\n",
       "       [0.4873215 ],\n",
       "       [0.51430434],\n",
       "       [0.43261182],\n",
       "       [0.465976  ],\n",
       "       [0.46207035],\n",
       "       [0.46382987],\n",
       "       [0.51430434],\n",
       "       [0.47495428],\n",
       "       [0.5109911 ],\n",
       "       [0.44724253],\n",
       "       [0.56253314],\n",
       "       [0.50543046],\n",
       "       [0.45600164],\n",
       "       [0.47398955],\n",
       "       [0.45706832],\n",
       "       [0.5064504 ],\n",
       "       [0.49552912],\n",
       "       [0.47422287],\n",
       "       [0.4696939 ],\n",
       "       [0.5305292 ],\n",
       "       [0.4446647 ],\n",
       "       [0.52715904],\n",
       "       [0.4128046 ],\n",
       "       [0.51589394],\n",
       "       [0.47398955],\n",
       "       [0.45023474],\n",
       "       [0.47312194],\n",
       "       [0.46913004],\n",
       "       [0.5167372 ],\n",
       "       [0.51351875],\n",
       "       [0.4696939 ],\n",
       "       [0.48056632],\n",
       "       [0.51364046],\n",
       "       [0.52458817],\n",
       "       [0.59822875],\n",
       "       [0.5246509 ],\n",
       "       [0.52559304],\n",
       "       [0.4331243 ],\n",
       "       [0.4954975 ],\n",
       "       [0.47312194],\n",
       "       [0.5404451 ],\n",
       "       [0.49871898],\n",
       "       [0.48135033],\n",
       "       [0.48626363],\n",
       "       [0.5150398 ],\n",
       "       [0.41486254],\n",
       "       [0.4446647 ],\n",
       "       [0.52715904],\n",
       "       [0.47312194],\n",
       "       [0.45730573],\n",
       "       [0.465976  ],\n",
       "       [0.5555877 ],\n",
       "       [0.4926367 ],\n",
       "       [0.46574724],\n",
       "       [0.42516395],\n",
       "       [0.48253596],\n",
       "       [0.47037333],\n",
       "       [0.47669426],\n",
       "       [0.4602645 ],\n",
       "       [0.51134866],\n",
       "       [0.5308322 ],\n",
       "       [0.5457104 ],\n",
       "       [0.5165024 ],\n",
       "       [0.46654812],\n",
       "       [0.44309375],\n",
       "       [0.57896125],\n",
       "       [0.49856782],\n",
       "       [0.51134866],\n",
       "       [0.48626363],\n",
       "       [0.52458817],\n",
       "       [0.48679346],\n",
       "       [0.59822875],\n",
       "       [0.46666965],\n",
       "       [0.48065796],\n",
       "       [0.52715904],\n",
       "       [0.4682665 ],\n",
       "       [0.43664426],\n",
       "       [0.43713602],\n",
       "       [0.48156637],\n",
       "       [0.45952934],\n",
       "       [0.5555877 ],\n",
       "       [0.5337175 ],\n",
       "       [0.47312817],\n",
       "       [0.46192613],\n",
       "       [0.45456043],\n",
       "       [0.46654812],\n",
       "       [0.45028076],\n",
       "       [0.47906062],\n",
       "       [0.4864186 ],\n",
       "       [0.5555877 ],\n",
       "       [0.51134866],\n",
       "       [0.46608827],\n",
       "       [0.47669426],\n",
       "       [0.50232726],\n",
       "       [0.43713602],\n",
       "       [0.4343085 ],\n",
       "       [0.46207035],\n",
       "       [0.43713602],\n",
       "       [0.43246296],\n",
       "       [0.46654812],\n",
       "       [0.47523627],\n",
       "       [0.5555877 ],\n",
       "       [0.5253963 ],\n",
       "       [0.49856782],\n",
       "       [0.47312817],\n",
       "       [0.43261182],\n",
       "       [0.52042836],\n",
       "       [0.4971311 ],\n",
       "       [0.5314845 ],\n",
       "       [0.48065796],\n",
       "       [0.4682665 ],\n",
       "       [0.44309375],\n",
       "       [0.57896125],\n",
       "       [0.4902206 ],\n",
       "       [0.43664426],\n",
       "       [0.5616788 ],\n",
       "       [0.56529766],\n",
       "       [0.48504698],\n",
       "       [0.5280425 ],\n",
       "       [0.48065796],\n",
       "       [0.5253963 ],\n",
       "       [0.4766129 ],\n",
       "       [0.48837805],\n",
       "       [0.43713602],\n",
       "       [0.45565933],\n",
       "       [0.46207035],\n",
       "       [0.46913004],\n",
       "       [0.47646064],\n",
       "       [0.52458817],\n",
       "       [0.5011221 ],\n",
       "       [0.5109911 ],\n",
       "       [0.48107827],\n",
       "       [0.48156637],\n",
       "       [0.47669426],\n",
       "       [0.45548707],\n",
       "       [0.47292417],\n",
       "       [0.43713602],\n",
       "       [0.5445471 ],\n",
       "       [0.46207035],\n",
       "       [0.43713602],\n",
       "       [0.51076496],\n",
       "       [0.5253963 ],\n",
       "       [0.52559304],\n",
       "       [0.43713602],\n",
       "       [0.51553875],\n",
       "       [0.48065796],\n",
       "       [0.4971311 ],\n",
       "       [0.48065796],\n",
       "       [0.45730573],\n",
       "       [0.5555877 ],\n",
       "       [0.52179116],\n",
       "       [0.49835742],\n",
       "       [0.43713602],\n",
       "       [0.5024025 ],\n",
       "       [0.46207035],\n",
       "       [0.51134866],\n",
       "       [0.44319838],\n",
       "       [0.49435762],\n",
       "       [0.4366546 ],\n",
       "       [0.4689108 ],\n",
       "       [0.48786774],\n",
       "       [0.47266006],\n",
       "       [0.49871898],\n",
       "       [0.47823068],\n",
       "       [0.4498023 ],\n",
       "       [0.51847875],\n",
       "       [0.51134866],\n",
       "       [0.45023474],\n",
       "       [0.41880763],\n",
       "       [0.46654812],\n",
       "       [0.47312817],\n",
       "       [0.48107827],\n",
       "       [0.5097087 ],\n",
       "       [0.48889711],\n",
       "       [0.4446647 ],\n",
       "       [0.54508114],\n",
       "       [0.46207035],\n",
       "       [0.43713602],\n",
       "       [0.52988863],\n",
       "       [0.46207035],\n",
       "       [0.43713602],\n",
       "       [0.46884528],\n",
       "       [0.51364046],\n",
       "       [0.46654812],\n",
       "       [0.49835742],\n",
       "       [0.47381744],\n",
       "       [0.47312817],\n",
       "       [0.4891309 ],\n",
       "       [0.43713602],\n",
       "       [0.47381744],\n",
       "       [0.47523627],\n",
       "       [0.47823068],\n",
       "       [0.52179116],\n",
       "       [0.41880763],\n",
       "       [0.48051104],\n",
       "       [0.48065796],\n",
       "       [0.52715904],\n",
       "       [0.46287927],\n",
       "       [0.46207035],\n",
       "       [0.45171994],\n",
       "       [0.46654812],\n",
       "       [0.45090908],\n",
       "       [0.59822875],\n",
       "       [0.43713602],\n",
       "       [0.46884528],\n",
       "       [0.48339885],\n",
       "       [0.48889711],\n",
       "       [0.48339885],\n",
       "       [0.43713602],\n",
       "       [0.45548707],\n",
       "       [0.46207035],\n",
       "       [0.48889711],\n",
       "       [0.50232726],\n",
       "       [0.5185754 ],\n",
       "       [0.49871898],\n",
       "       [0.44973794],\n",
       "       [0.4864186 ],\n",
       "       [0.43713602],\n",
       "       [0.48722488],\n",
       "       [0.46654812],\n",
       "       [0.4511219 ],\n",
       "       [0.43664426],\n",
       "       [0.5100931 ],\n",
       "       [0.51364046],\n",
       "       [0.46654812],\n",
       "       [0.47866642],\n",
       "       [0.48107827],\n",
       "       [0.5058681 ],\n",
       "       [0.4602645 ],\n",
       "       [0.48339885],\n",
       "       [0.43713602],\n",
       "       [0.4784923 ],\n",
       "       [0.47398955],\n",
       "       [0.52559304],\n",
       "       [0.48889711],\n",
       "       [0.54603726],\n",
       "       [0.48339885],\n",
       "       [0.4682665 ],\n",
       "       [0.51134866],\n",
       "       [0.47297212],\n",
       "       [0.46207035],\n",
       "       [0.521338  ],\n",
       "       [0.4930801 ],\n",
       "       [0.4569947 ],\n",
       "       [0.46287927],\n",
       "       [0.46207035],\n",
       "       [0.47365403],\n",
       "       [0.5015287 ],\n",
       "       [0.59822875],\n",
       "       [0.48107827],\n",
       "       [0.3854224 ],\n",
       "       [0.5246509 ],\n",
       "       [0.51134866],\n",
       "       [0.47297212],\n",
       "       [0.46207035],\n",
       "       [0.4606963 ],\n",
       "       [0.48065796],\n",
       "       [0.47495428],\n",
       "       [0.45365775],\n",
       "       [0.43561202],\n",
       "       [0.51553875],\n",
       "       [0.47881505],\n",
       "       [0.46654812],\n",
       "       [0.43713602],\n",
       "       [0.5314845 ],\n",
       "       [0.48253596],\n",
       "       [0.48339885],\n",
       "       [0.4910293 ],\n",
       "       [0.45251456],\n",
       "       [0.43713602],\n",
       "       [0.47823068],\n",
       "       [0.4499187 ],\n",
       "       [0.48889711],\n",
       "       [0.47128662],\n",
       "       [0.49407688],\n",
       "       [0.46932504],\n",
       "       [0.47523627],\n",
       "       [0.43713602],\n",
       "       [0.5796969 ],\n",
       "       [0.51553875],\n",
       "       [0.5253963 ],\n",
       "       [0.48065796],\n",
       "       [0.48156637],\n",
       "       [0.46654812],\n",
       "       [0.54720956],\n",
       "       [0.47523627],\n",
       "       [0.47128662],\n",
       "       [0.5024025 ],\n",
       "       [0.43246296],\n",
       "       [0.45251456],\n",
       "       [0.5295497 ],\n",
       "       [0.5311813 ],\n",
       "       [0.48107827],\n",
       "       [0.51134866],\n",
       "       [0.47312817],\n",
       "       [0.45365775],\n",
       "       [0.45456043],\n",
       "       [0.51364046],\n",
       "       [0.5295497 ],\n",
       "       [0.45548707],\n",
       "       [0.5058681 ],\n",
       "       [0.50719976],\n",
       "       [0.47669426],\n",
       "       [0.43713602],\n",
       "       [0.46884528],\n",
       "       [0.48107827],\n",
       "       [0.42139122],\n",
       "       [0.44319838],\n",
       "       [0.5015287 ],\n",
       "       [0.47037333],\n",
       "       [0.51364046],\n",
       "       [0.48692897],\n",
       "       [0.52559304],\n",
       "       [0.5121058 ],\n",
       "       [0.52559304],\n",
       "       [0.5295497 ],\n",
       "       [0.47115704],\n",
       "       [0.43337497],\n",
       "       [0.48889711],\n",
       "       [0.4926367 ],\n",
       "       [0.5058681 ],\n",
       "       [0.46207035],\n",
       "       [0.43713602],\n",
       "       [0.51364046],\n",
       "       [0.52289397],\n",
       "       [0.49871898],\n",
       "       [0.51553875],\n",
       "       [0.43713602],\n",
       "       [0.52974784],\n",
       "       [0.49052304],\n",
       "       [0.43713602],\n",
       "       [0.52708864],\n",
       "       [0.46654812],\n",
       "       [0.41797185],\n",
       "       [0.4569947 ],\n",
       "       [0.5305292 ],\n",
       "       [0.43713602],\n",
       "       [0.43787882],\n",
       "       [0.49871898],\n",
       "       [0.49997348],\n",
       "       [0.45365775],\n",
       "       [0.46192613],\n",
       "       [0.45090908],\n",
       "       [0.46192613],\n",
       "       [0.48107827],\n",
       "       [0.51847875],\n",
       "       [0.43713602],\n",
       "       [0.50719976],\n",
       "       [0.4663061 ],\n",
       "       [0.46523356],\n",
       "       [0.52559304],\n",
       "       [0.5576247 ],\n",
       "       [0.48643237],\n",
       "       [0.45090908],\n",
       "       [0.49183685],\n",
       "       [0.46523356],\n",
       "       [0.49552912],\n",
       "       [0.54001087],\n",
       "       [0.48065796],\n",
       "       [0.45706832],\n",
       "       [0.48339885],\n",
       "       [0.5043884 ],\n",
       "       [0.45973274],\n",
       "       [0.49871898],\n",
       "       [0.43713602],\n",
       "       [0.46884528],\n",
       "       [0.46287927],\n",
       "       [0.46207035],\n",
       "       [0.45171994],\n",
       "       [0.5108957 ],\n",
       "       [0.51847875],\n",
       "       [0.56926525],\n",
       "       [0.4602645 ],\n",
       "       [0.4182169 ],\n",
       "       [0.52988863],\n",
       "       [0.48065796],\n",
       "       [0.45952934],\n",
       "       [0.4966608 ],\n",
       "       [0.4602645 ],\n",
       "       [0.4182169 ],\n",
       "       [0.4584928 ],\n",
       "       [0.45365775],\n",
       "       [0.48643237],\n",
       "       [0.51134866],\n",
       "       [0.39360288],\n",
       "       [0.46884528],\n",
       "       [0.47523627],\n",
       "       [0.5508482 ],\n",
       "       [0.4773991 ],\n",
       "       [0.55826694],\n",
       "       [0.5097087 ],\n",
       "       [0.54123455],\n",
       "       [0.54123455],\n",
       "       [0.49294508],\n",
       "       [0.5138986 ],\n",
       "       [0.50719976],\n",
       "       [0.5097069 ],\n",
       "       [0.46913004],\n",
       "       [0.48339885],\n",
       "       [0.4891309 ],\n",
       "       [0.42465767],\n",
       "       [0.49871898],\n",
       "       [0.47823068],\n",
       "       [0.4610694 ],\n",
       "       [0.5362276 ],\n",
       "       [0.50031954],\n",
       "       [0.42080197],\n",
       "       [0.43351942],\n",
       "       [0.4784923 ],\n",
       "       [0.43713602],\n",
       "       [0.46884528],\n",
       "       [0.49552912],\n",
       "       [0.4602645 ],\n",
       "       [0.43713602],\n",
       "       [0.5445471 ],\n",
       "       [0.48065796],\n",
       "       [0.43351942],\n",
       "       [0.48051104],\n",
       "       [0.5138986 ],\n",
       "       [0.47823068],\n",
       "       [0.52179116],\n",
       "       [0.46207035],\n",
       "       [0.43713602],\n",
       "       [0.46666965],\n",
       "       [0.4966608 ],\n",
       "       [0.46884528],\n",
       "       [0.5295497 ],\n",
       "       [0.51430434],\n",
       "       [0.56253314],\n",
       "       [0.52559304],\n",
       "       [0.45090908],\n",
       "       [0.47012186],\n",
       "       [0.47523627],\n",
       "       [0.44667614],\n",
       "       [0.59080464],\n",
       "       [0.43276483],\n",
       "       [0.48107827],\n",
       "       [0.44297972],\n",
       "       [0.41880763],\n",
       "       [0.48833472],\n",
       "       [0.48392358],\n",
       "       [0.47312194],\n",
       "       [0.52559304],\n",
       "       [0.4331243 ],\n",
       "       [0.45152867],\n",
       "       [0.45171994],\n",
       "       [0.52559304],\n",
       "       [0.4092056 ],\n",
       "       [0.42516395],\n",
       "       [0.4766587 ],\n",
       "       [0.52179116],\n",
       "       [0.51134866],\n",
       "       [0.5065078 ],\n",
       "       [0.5090784 ],\n",
       "       [0.5445471 ],\n",
       "       [0.46207035],\n",
       "       [0.44772577],\n",
       "       [0.4446647 ],\n",
       "       [0.41976458],\n",
       "       [0.5018458 ],\n",
       "       [0.48107827],\n",
       "       [0.5796969 ],\n",
       "       [0.46207035],\n",
       "       [0.43713602],\n",
       "       [0.48065796],\n",
       "       [0.48065796],\n",
       "       [0.47037333],\n",
       "       [0.4602645 ],\n",
       "       [0.47823068],\n",
       "       [0.54720956],\n",
       "       [0.5314845 ],\n",
       "       [0.4689108 ],\n",
       "       [0.4891309 ],\n",
       "       [0.43713602],\n",
       "       [0.5502472 ],\n",
       "       [0.45456043],\n",
       "       [0.55249876],\n",
       "       [0.48156637],\n",
       "       [0.51134866],\n",
       "       [0.44772577],\n",
       "       [0.43261182],\n",
       "       [0.43713602],\n",
       "       [0.5298271 ],\n",
       "       [0.5305292 ],\n",
       "       [0.4966608 ],\n",
       "       [0.48107827],\n",
       "       [0.48107827],\n",
       "       [0.46884528],\n",
       "       [0.48065796],\n",
       "       [0.51134866],\n",
       "       [0.47943068],\n",
       "       [0.46207035],\n",
       "       [0.45952934],\n",
       "       [0.45205435],\n",
       "       [0.43713602],\n",
       "       [0.45129013],\n",
       "       [0.4543076 ],\n",
       "       [0.4602645 ],\n",
       "       [0.5064504 ],\n",
       "       [0.47823068],\n",
       "       [0.52179116],\n",
       "       [0.45992136],\n",
       "       [0.47381744],\n",
       "       [0.5730263 ],\n",
       "       [0.47312194],\n",
       "       [0.43713602],\n",
       "       [0.51417863],\n",
       "       [0.48065796],\n",
       "       [0.458501  ]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import hashing_trick\n",
    "\n",
    "# reads in text as list \n",
    "file = open(\"gotS1R1.txt\", \"r\")\n",
    "review = file.readlines()\n",
    "\n",
    "#turns list into string \n",
    "data = ''.join(review)\n",
    "\n",
    "# gets rid of punctuation and makes it lower case\n",
    "for char in '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~\\n':\n",
    "    data = data.replace(char,' ')\n",
    "data = data.strip(\"\\ufeff\")\n",
    "data = data.lower()\n",
    "\n",
    "# turns string array into integer array\n",
    "testt_data = hashing_trick(data, 500, hash_function='md5')\n",
    "\n",
    "# vectorizes integer array\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results\n",
    "\n",
    "x_testt = vectorize_sequences(testt_data)\n",
    "\n",
    "# produces resulting data\n",
    "model.predict(x_testt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input arrays should have the same number of samples as target arrays. Found 826 input samples and 25000 target samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-bc660d7fb9f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_testt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    802\u001b[0m             ]\n\u001b[1;32m    803\u001b[0m             \u001b[0;31m# Check that all arrays have the same length.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m             \u001b[0mcheck_array_length_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m                 \u001b[0;31m# Additional checks to avoid users mistakenly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_array_length_consistency\u001b[0;34m(inputs, targets, weights)\u001b[0m\n\u001b[1;32m    235\u001b[0m                          \u001b[0;34m'the same number of samples as target arrays. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                          \u001b[0;34m'Found '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' input samples '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m                          'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         raise ValueError('All sample_weight arrays should have '\n",
      "\u001b[0;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 826 input samples and 25000 target samples."
     ]
    }
   ],
   "source": [
    "results = model.evaluate(x_testt, y_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import hashing_trick\n",
    "\n",
    "# reads in text as list \n",
    "file = open(\"gotS1R2.txt\", \"r\")\n",
    "review = file.readlines()\n",
    "\n",
    "#turns list into string \n",
    "data = ''.join(review)\n",
    "\n",
    "# gets rid of punctuation and makes it lower case\n",
    "for char in '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~\\n':\n",
    "    data = data.replace(char,' ')\n",
    "data = data.strip(\"\\ufeff\")\n",
    "data = data.lower()\n",
    "\n",
    "# turns string array into integer array\n",
    "testt_data = hashing_trick(data, 500, hash_function='md5')\n",
    "\n",
    "# vectorizes integer array\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results\n",
    "\n",
    "x_testt = vectorize_sequences(testt_data)\n",
    "\n",
    "# produces resulting data\n",
    "results\n",
    "model.predict(x_testt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import hashing_trick\n",
    "\n",
    "# reads in text as list \n",
    "file = open(\"gotS2R1.txt\", \"r\")\n",
    "review = file.readlines()\n",
    "\n",
    "#turns list into string \n",
    "data = ''.join(review)\n",
    "\n",
    "# gets rid of punctuation and makes it lower case\n",
    "for char in '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~\\n':\n",
    "    data = data.replace(char,' ')\n",
    "data = data.strip(\"\\ufeff\")\n",
    "data = data.lower()\n",
    "\n",
    "# turns string array into integer array\n",
    "testt_data = hashing_trick(data, 500, hash_function='md5')\n",
    "\n",
    "# vectorizes integer array\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results\n",
    "\n",
    "x_testt = vectorize_sequences(testt_data)\n",
    "\n",
    "# produces resulting data\n",
    "results\n",
    "model.predict(x_testt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import hashing_trick\n",
    "\n",
    "# reads in text as list \n",
    "file = open(\"gotS2R2.txt\", \"r\")\n",
    "review = file.readlines()\n",
    "\n",
    "#turns list into string \n",
    "data = ''.join(review)\n",
    "\n",
    "# gets rid of punctuation and makes it lower case\n",
    "for char in '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~\\n':\n",
    "    data = data.replace(char,' ')\n",
    "data = data.strip(\"\\ufeff\")\n",
    "data = data.lower()\n",
    "\n",
    "# turns string array into integer array\n",
    "testt_data = hashing_trick(data, 500, hash_function='md5')\n",
    "\n",
    "# vectorizes integer array\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results\n",
    "\n",
    "x_testt = vectorize_sequences(testt_data)\n",
    "\n",
    "# produces resulting data\n",
    "results\n",
    "model.predict(x_testt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import hashing_trick\n",
    "\n",
    "# reads in text as list \n",
    "file = open(\"gotS3R1.txt\", \"r\")\n",
    "review = file.readlines()\n",
    "\n",
    "#turns list into string \n",
    "data = ''.join(review)\n",
    "\n",
    "# gets rid of punctuation and makes it lower case\n",
    "for char in '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~\\n':\n",
    "    data = data.replace(char,' ')\n",
    "data = data.strip(\"\\ufeff\")\n",
    "data = data.lower()\n",
    "\n",
    "# turns string array into integer array\n",
    "testt_data = hashing_trick(data, 500, hash_function='md5')\n",
    "\n",
    "# vectorizes integer array\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results\n",
    "\n",
    "x_testt = vectorize_sequences(testt_data)\n",
    "\n",
    "# produces resulting data\n",
    "results\n",
    "model.predict(x_testt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import hashing_trick\n",
    "\n",
    "# reads in text as list \n",
    "file = open(\"gotS3R2.txt\", \"r\")\n",
    "review = file.readlines()\n",
    "\n",
    "#turns list into string \n",
    "data = ''.join(review)\n",
    "\n",
    "# gets rid of punctuation and makes it lower case\n",
    "for char in '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~\\n':\n",
    "    data = data.replace(char,' ')\n",
    "data = data.strip(\"\\ufeff\")\n",
    "data = data.lower()\n",
    "\n",
    "# turns string array into integer array\n",
    "testt_data = hashing_trick(data, 500, hash_function='md5')\n",
    "\n",
    "# vectorizes integer array\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results\n",
    "\n",
    "x_testt = vectorize_sequences(testt_data)\n",
    "\n",
    "# produces resulting data\n",
    "results\n",
    "model.predict(x_testt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import hashing_trick\n",
    "\n",
    "# reads in text as list \n",
    "file = open(\"gotS4R1.txt\", \"r\")\n",
    "review = file.readlines()\n",
    "\n",
    "#turns list into string \n",
    "data = ''.join(review)\n",
    "\n",
    "# gets rid of punctuation and makes it lower case\n",
    "for char in '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~\\n':\n",
    "    data = data.replace(char,' ')\n",
    "data = data.strip(\"\\ufeff\")\n",
    "data = data.lower()\n",
    "\n",
    "# turns string array into integer array\n",
    "testt_data = hashing_trick(data, 500, hash_function='md5')\n",
    "\n",
    "# vectorizes integer array\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results\n",
    "\n",
    "x_testt = vectorize_sequences(testt_data)\n",
    "\n",
    "# produces resulting data\n",
    "results\n",
    "model.predict(x_testt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import hashing_trick\n",
    "\n",
    "# reads in text as list \n",
    "file = open(\"gotS4R2.txt\", \"r\")\n",
    "review = file.readlines()\n",
    "\n",
    "#turns list into string \n",
    "data = ''.join(review)\n",
    "\n",
    "# gets rid of punctuation and makes it lower case\n",
    "for char in '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~\\n':\n",
    "    data = data.replace(char,' ')\n",
    "data = data.strip(\"\\ufeff\")\n",
    "data = data.lower()\n",
    "\n",
    "# turns string array into integer array\n",
    "testt_data = hashing_trick(data, 500, hash_function='md5')\n",
    "\n",
    "# vectorizes integer array\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results\n",
    "\n",
    "x_testt = vectorize_sequences(testt_data)\n",
    "\n",
    "# produces resulting data\n",
    "results\n",
    "model.predict(x_testt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import hashing_trick\n",
    "\n",
    "# reads in text as list \n",
    "file = open(\"gotS5R1.txt\", \"r\")\n",
    "review = file.readlines()\n",
    "\n",
    "#turns list into string \n",
    "data = ''.join(review)\n",
    "\n",
    "# gets rid of punctuation and makes it lower case\n",
    "for char in '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~\\n':\n",
    "    data = data.replace(char,' ')\n",
    "data = data.strip(\"\\ufeff\")\n",
    "data = data.lower()\n",
    "\n",
    "# turns string array into integer array\n",
    "testt_data = hashing_trick(data, 500, hash_function='md5')\n",
    "\n",
    "# vectorizes integer array\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results\n",
    "\n",
    "x_testt = vectorize_sequences(testt_data)\n",
    "\n",
    "# produces resulting data\n",
    "results\n",
    "model.predict(x_testt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import hashing_trick\n",
    "\n",
    "# reads in text as list \n",
    "file = open(\"gotS5R2.txt\", \"r\")\n",
    "review = file.readlines()\n",
    "\n",
    "#turns list into string \n",
    "data = ''.join(review)\n",
    "\n",
    "# gets rid of punctuation and makes it lower case\n",
    "for char in '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~\\n':\n",
    "    data = data.replace(char,' ')\n",
    "data = data.strip(\"\\ufeff\")\n",
    "data = data.lower()\n",
    "\n",
    "# turns string array into integer array\n",
    "testt_data = hashing_trick(data, 500, hash_function='md5')\n",
    "\n",
    "# vectorizes integer array\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results\n",
    "\n",
    "x_testt = vectorize_sequences(testt_data)\n",
    "\n",
    "# produces resulting data\n",
    "results\n",
    "model.predict(x_testt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import hashing_trick\n",
    "\n",
    "# reads in text as list \n",
    "file = open(\"gotS6R1.txt\", \"r\")\n",
    "review = file.readlines()\n",
    "\n",
    "#turns list into string \n",
    "data = ''.join(review)\n",
    "\n",
    "# gets rid of punctuation and makes it lower case\n",
    "for char in '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~\\n':\n",
    "    data = data.replace(char,' ')\n",
    "data = data.strip(\"\\ufeff\")\n",
    "data = data.lower()\n",
    "\n",
    "# turns string array into integer array\n",
    "testt_data = hashing_trick(data, 500, hash_function='md5')\n",
    "\n",
    "# vectorizes integer array\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results\n",
    "\n",
    "x_testt = vectorize_sequences(testt_data)\n",
    "\n",
    "# produces resulting data\n",
    "results\n",
    "model.predict(x_testt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import hashing_trick\n",
    "\n",
    "# reads in text as list \n",
    "file = open(\"gotS6R2.txt\", \"r\")\n",
    "review = file.readlines()\n",
    "\n",
    "#turns list into string \n",
    "data = ''.join(review)\n",
    "\n",
    "# gets rid of punctuation and makes it lower case\n",
    "for char in '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~\\n':\n",
    "    data = data.replace(char,' ')\n",
    "data = data.strip(\"\\ufeff\")\n",
    "data = data.lower()\n",
    "\n",
    "# turns string array into integer array\n",
    "testt_data = hashing_trick(data, 500, hash_function='md5')\n",
    "\n",
    "# vectorizes integer array\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results\n",
    "\n",
    "x_testt = vectorize_sequences(testt_data)\n",
    "\n",
    "# produces resulting data\n",
    "results\n",
    "model.predict(x_testt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import hashing_trick\n",
    "\n",
    "# reads in text as list \n",
    "file = open(\"gotS7R1.txt\", \"r\")\n",
    "review = file.readlines()\n",
    "\n",
    "#turns list into string \n",
    "data = ''.join(review)\n",
    "\n",
    "# gets rid of punctuation and makes it lower case\n",
    "for char in '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~\\n':\n",
    "    data = data.replace(char,' ')\n",
    "data = data.strip(\"\\ufeff\")\n",
    "data = data.lower()\n",
    "\n",
    "# turns string array into integer array\n",
    "testt_data = hashing_trick(data, 500, hash_function='md5')\n",
    "\n",
    "# vectorizes integer array\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results\n",
    "\n",
    "x_testt = vectorize_sequences(testt_data)\n",
    "\n",
    "# produces resulting data\n",
    "\n",
    "model.predict(x_testt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import hashing_trick\n",
    "\n",
    "# reads in text as list \n",
    "file = open(\"gotS7R2.txt\", \"r\")\n",
    "review = file.readlines()\n",
    "\n",
    "#turns list into string \n",
    "data = ''.join(review)\n",
    "\n",
    "# gets rid of punctuation and makes it lower case\n",
    "for char in '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~\\n':\n",
    "    data = data.replace(char,' ')\n",
    "data = data.strip(\"\\ufeff\")\n",
    "data = data.lower()\n",
    "\n",
    "# turns string array into integer array\n",
    "testt_data = hashing_trick(data, 500, hash_function='md5')\n",
    "\n",
    "# vectorizes integer array\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results\n",
    "\n",
    "x_testt = vectorize_sequences(testt_data)\n",
    "\n",
    "# produces resulting data\n",
    "results\n",
    "model.predict(x_testt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
